{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTQa4gMgl8BokoboXiyQlK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcKjSszvobMo"
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "from networks import lpips\n",
        "from networks import deeplab_xception_transfer\n",
        "from networks.style_gan_2 import Generator\n",
        "from networks.graphonomy_inference import get_mask\n",
        "from losses.style_loss import StyleLoss\n",
        "from losses.appearance_loss import AppearanceLoss\n",
        "from losses.noise_loss import noise_regularize, noise_normalize_\n",
        "from datasets.ffhq import process_image, dilate_erosion_mask\n",
        "from utils import optimizer_utils, image_utils\n",
        "\n",
        "\n",
        "def get_lr(t, initial_lr, rampdown=0.25, rampup=0.05):\n",
        "    lr_ramp = min(1, (1 - t) / rampdown)\n",
        "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
        "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
        "\n",
        "    return initial_lr * lr_ramp\n",
        "\n",
        "\n",
        "def latent_noise(latent, strength):\n",
        "    noise = torch.randn_like(latent) * strength\n",
        "\n",
        "    return latent + noise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--size\", type=int, default=1024)\n",
        "    parser.add_argument(\"--lr_rampup\", type=float, default=0.05)\n",
        "    parser.add_argument(\"--lr_rampdown\", type=float, default=0.25)\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--noise\", type=float, default=0.05)\n",
        "    parser.add_argument(\"--noise_ramp\", type=float, default=0.75)\n",
        "    parser.add_argument(\"--step\", type=int, default=2000)\n",
        "    parser.add_argument(\"--save_synth_every\", type=int, default=500)\n",
        "    parser.add_argument(\"--save_pickle\", type=int, default=0)\n",
        "    parser.add_argument(\"--noise_regularize\", type=float, default=1e5)\n",
        "    parser.add_argument(\"--image1\", type=str, default=\"00018.png\")\n",
        "    parser.add_argument(\"--image2\", type=str, default=\"00200.png\")\n",
        "    parser.add_argument(\"--image3\", type=str, default=\"00079.png\")\n",
        "    parser.add_argument(\"--use_GO\", type=int, default=1, help=\"Use GO or no-GO\")\n",
        "    parser.add_argument(\"--style_mask_type\", type=int, default=1, help=\"1.ori, 2.comp. mask\")\n",
        "    parser.add_argument(\"--lpips_vgg_blocks\", type=str, default=\"4,5\")\n",
        "    parser.add_argument(\"--style_vgg_layers\", type=str, default=\"3,8,15,22\")\n",
        "    parser.add_argument(\"--appearance_vgg_layers\", type=str, default=\"1\")\n",
        "    parser.add_argument(\"--lambda_facerec\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--lambda_hairstyle\", type=float, default=15000.0)\n",
        "    parser.add_argument(\"--lambda_hairappearance\", type=float, default=40.0)\n",
        "    parser.add_argument(\"--lambda_hairrec\", type=float, default=1.0)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    n_mean_latent = 10000\n",
        "\n",
        "    resize = min(args.size, 256)\n",
        "\n",
        "    ############################## PREPARE DATA\n",
        "\n",
        "    # Define paths to tuples and checkpoints\n",
        "    raw = \"data/images\"\n",
        "    mask = \"data/masks\"\n",
        "    background = \"data/backgrounds\"\n",
        "    softmask = \"data/softmasks\"\n",
        "    input_name = (\n",
        "        args.image1.split(\".\")[0] + \"_\"\n",
        "        + args.image2.split(\".\")[0] + \"_\"\n",
        "        + args.image3.split(\".\")[0]\n",
        "    )\n",
        "    dest = os.path.join(\"data/results\", input_name)\n",
        "    if not os.path.exists(dest): os.makedirs(dest)\n",
        "    styleganv2_ckpt_path = '/content/drive/MyDrive/stylegan2-ffhq-config-f.pt'\n",
        "    graphonomy_model_path = '/content/drive/MyDrive/inference.pth'\n",
        "\n",
        "    # Get path to image files\n",
        "    image_files = image_utils.getImagePaths(\n",
        "        raw, mask, background, args.image1, args.image2, args.image3\n",
        "    )\n",
        "\n",
        "    # Get images and masks \n",
        "    I_1, M_1, HM_1, H_1, FM_1, F_1, FG_1 = process_image(\n",
        "        image_files['I_1_path'], image_files['M_1_path'], size=resize, normalize=1\n",
        "    )\n",
        "    I_2, M_2, HM_2, H_2, FM_2, F_2, FG_2 = process_image(\n",
        "        image_files['I_2_path'], image_files['M_2_path'], size=resize, normalize=1\n",
        "    )\n",
        "    I_3, M_3, HM_3, H_3, FM_3, F_3, FG_3 = process_image(\n",
        "        image_files['I_3_path'], image_files['M_3_path'], size=resize, normalize=1\n",
        "    )\n",
        "\n",
        "    # Make cuda\n",
        "    I_1, M_1, HM_1, H_1, FM_1, F_1, FG_1 = optimizer_utils.make_cuda(\n",
        "        [I_1, M_1, HM_1, H_1, FM_1, F_1, FG_1]\n",
        "    )\n",
        "    I_2, M_2, HM_2, H_2, FM_2, F_2, FG_2 = optimizer_utils.make_cuda(\n",
        "        [I_2, M_2, HM_2, H_2, FM_2, F_2, FG_2]\n",
        "    )\n",
        "    I_3, M_3, HM_3, H_3, FM_3, F_3, FG_3 = optimizer_utils.make_cuda(\n",
        "        [I_3, M_3, HM_3, H_3, FM_3, F_3, FG_3]\n",
        "    )\n",
        "\n",
        "    # Expand batch dim\n",
        "    I_1, M_1, HM_1, H_1, FM_1, F_1, FG_1 = image_utils.addBatchDim(\n",
        "        [I_1, M_1, HM_1, H_1, FM_1, F_1, FG_1]\n",
        "    )\n",
        "    I_2, M_2, HM_2, H_2, FM_2, F_2, FG_2 = image_utils.addBatchDim(\n",
        "        [I_2, M_2, HM_2, H_2, FM_2, F_2, FG_2]\n",
        "    )\n",
        "    I_3, M_3, HM_3, H_3 = image_utils.addBatchDim(\n",
        "        [I_3, M_3, HM_3, H_3]\n",
        "    )\n",
        "\n",
        "    HM_2D, HM_2E = dilate_erosion_mask(image_files['M_2_path'], resize)\n",
        "    HM_2D, HM_2E = HM_2D.float(), HM_2E.float()\n",
        "    HM_2D, HM_2E = optimizer_utils.make_cuda([HM_2D, HM_2E])\n",
        "    HM_2D, HM_2E = image_utils.addBatchDim([HM_2D, HM_2E])\n",
        "    ignore_region = HM_2D - HM_2E\n",
        "    mask_loss_region = 1 - ignore_region\n",
        "\n",
        "    # Write masks to disk for visualization\n",
        "    image_utils.writeMaskToDisk(\n",
        "        [HM_1, HM_2, HM_2D, HM_2E, ignore_region],\n",
        "        ['HM_1.png', 'HM_2.png', 'HM_2D.png', 'HM_2E.png', 'ignore_region.png'],\n",
        "        dest\n",
        "    )\n",
        "    image_utils.writeImageToDisk(\n",
        "        [I_1.clone(), I_2.clone(), I_3.clone()], ['I_1.png', 'I_2.png', 'I_3.png'], dest\n",
        "    )\n",
        "\n",
        "    ############################# LOAD MODELS\n",
        "\n",
        "    lpips_vgg_blocks = args.lpips_vgg_blocks.split(',')\n",
        "    # LPIPS MODEL\n",
        "    face_percept = lpips.PerceptualLoss(\n",
        "        model=\"net-lin\", net=\"vgg\", vgg_blocks=[\"1\", \"2\", \"3\", \"4\", \"5\"], use_gpu=device.startswith(\"cuda\") \n",
        "    )\n",
        "    hair_percept = lpips.PerceptualLoss(\n",
        "        model=\"net-lin\", net=\"vgg\", vgg_blocks=lpips_vgg_blocks, use_gpu=device.startswith(\"cuda\")\n",
        "    )\n",
        "\n",
        "    # STYLE + APPEARANCE MODEL\n",
        "    style_vgg_layers = args.style_vgg_layers.split(',')\n",
        "    style_vgg_layers = [int(i) for i in style_vgg_layers]\n",
        "    style = StyleLoss(\n",
        "        distance=\"l2\", VGG16_ACTIVATIONS_LIST=style_vgg_layers, normalize=False\n",
        "    )\n",
        "    style.cuda()\n",
        "\n",
        "    appearance_vgg_layers = args.appearance_vgg_layers.split(',')\n",
        "    appearance_vgg_layers = [int(i) for i in appearance_vgg_layers]\n",
        "    appearance = AppearanceLoss(\n",
        "        distance=\"l2\", VGG16_ACTIVATIONS_LIST=appearance_vgg_layers, normalize=False\n",
        "    )\n",
        "    appearance.cuda()\n",
        "\n",
        "    # GRAPHONOMY MODEL\n",
        "    net = deeplab_xception_transfer.deeplab_xception_transfer_projection_savemem(\n",
        "        n_classes=20,\n",
        "        hidden_layers=128,\n",
        "        source_classes=7,\n",
        "    )\n",
        "\n",
        "    state_dict = torch.load(graphonomy_model_path)\n",
        "    net.load_source_model(state_dict)\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    # GENERATOR\n",
        "    g_ema = Generator(args.size, 512, 8)\n",
        "    g_ema.load_state_dict(torch.load(styleganv2_ckpt_path)[\"g_ema\"], strict=False)\n",
        "    g_ema.eval()\n",
        "    g_ema = g_ema.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        noise_sample = torch.randn(n_mean_latent, 512, device=device)\n",
        "        latent_out = g_ema.style(noise_sample)\n",
        "\n",
        "        latent_mean = latent_out.mean(0)\n",
        "        latent_std = ((latent_out - latent_mean).pow(2).sum() / n_mean_latent) ** 0.5\n",
        "\n",
        "    noises_single = g_ema.make_noise()\n",
        "    noises = []\n",
        "    for noise in noises_single:\n",
        "        noises.append(noise.repeat(I_1.shape[0], 1, 1, 1).normal_())\n",
        "\n",
        "    latent_in = latent_mean.detach().clone().unsqueeze(0).repeat(I_1.shape[0], 1)\n",
        "    # copy over to W+\n",
        "    latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
        "    \n",
        "    latent_in.requires_grad = True\n",
        "\n",
        "    for noise in noises:\n",
        "        noise.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam([latent_in] + noises, lr=args.lr)\n",
        "\n",
        "    pbar = tqdm(range(args.step))\n",
        "    latent_path = []\n",
        "\n",
        "    losses_log = {\n",
        "        \"facerec\": [],\n",
        "        \"hairstyle_3g\": [],\n",
        "        \"hairstyle_2g\": [],\n",
        "        \"hairappearance_3g\": [],\n",
        "        \"hairappearance_2g\": [],\n",
        "        \"hairrec\": [],\n",
        "        \"noise\": [],\n",
        "    }\n",
        "    g_S2_vector_norms = {i: [] for i in range(latent_in.shape[1])}\n",
        "    g_S3_vector_norms = {i: [] for i in range(latent_in.shape[1])}\n",
        "    g_L_vector_norms = {i: [] for i in range(latent_in.shape[1])}\n",
        "    g_L_hat_vector_norms = {i: [] for i in range(latent_in.shape[1])}\n",
        "    dot_gL_gS2 = {i: [] for i in range(latent_in.shape[1])}\n",
        "    dot_gLhat_gS2 = {i: [] for i in range(latent_in.shape[1])}\n",
        "\n",
        "    for i in pbar:\n",
        "        t = i / args.step\n",
        "        lr = get_lr(t, args.lr)\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "        noise_strength = latent_std * args.noise * max(0, 1 - t / args.noise_ramp) ** 2\n",
        "        latent_n = latent_noise(latent_in, noise_strength.item())\n",
        "\n",
        "        I_G, _ = g_ema([latent_n], input_is_latent=True, noise=noises)\n",
        "\n",
        "        batch, channel, height, width = I_G.shape\n",
        "\n",
        "        if height > 256:\n",
        "            factor = height // 256\n",
        "\n",
        "            I_G = I_G.reshape(\n",
        "                batch, channel, height // factor, factor, width // factor, factor\n",
        "            )\n",
        "            I_G = I_G.mean([3, 5])\n",
        "\n",
        "        # get hair mask of synthesized image\n",
        "        predictions, HM_G, FM_G = get_mask(net, I_G)\n",
        "        HM_G = HM_G.float()\n",
        "        HM_G = torch.unsqueeze(HM_G, axis=0)\n",
        "        FM_G = torch.unsqueeze(FM_G, axis=0)\n",
        "\n",
        "        # LPIPS on face\n",
        "        target_mask = FM_1 * (1 - HM_2D)\n",
        "        facerec_loss = args.lambda_facerec * face_percept(I_G, I_1, mask=target_mask)\n",
        "        losses_log[\"facerec\"].append(facerec_loss.item())\n",
        "\n",
        "        # LPIPS on hair\n",
        "        hairrec_loss = args.lambda_hairrec * hair_percept(I_G, I_2, HM_2E)\n",
        "        losses_log[\"hairrec\"].append(hairrec_loss.item())\n",
        "\n",
        "        # Style Loss on hair\n",
        "        # compute target mask on synthesized image\n",
        "        if i < 1000:\n",
        "            if args.style_mask_type == 1:\n",
        "                mask2 = HM_G\n",
        "            elif args.style_mask_type == 2:\n",
        "                mask2 = HM_2 + (ignore_region * HM_G)\n",
        "                mask2 = torch.where(\n",
        "                    mask2 >= 1, torch.ones_like(mask2), torch.zeros_like(mask2)\n",
        "                )\n",
        "        H_G = I_G * mask2\n",
        "\n",
        "        # style loss between H_2 and H_G\n",
        "        hairstyle_2g_loss = args.lambda_hairstyle * style(\n",
        "            H_2, H_G, mask1=HM_2, mask2=mask2\n",
        "        )\n",
        "        hairappearance_2g_loss = args.lambda_hairappearance * appearance(\n",
        "            H_2, H_G, mask1=HM_2, mask2=mask2\n",
        "        )\n",
        "\n",
        "        # style loss between H_3 and H_G\n",
        "        hairstyle_3g_loss = args.lambda_hairstyle * style(\n",
        "            H_3, H_G, mask1=HM_3, mask2=mask2\n",
        "        )\n",
        "        hairappearance_3g_loss = args.lambda_hairappearance * appearance(\n",
        "            H_3, H_G, mask1=HM_3, mask2=mask2\n",
        "        )\n",
        "\n",
        "        losses_log[\"hairstyle_2g\"].append(hairstyle_2g_loss.item())\n",
        "        losses_log[\"hairstyle_3g\"].append(hairstyle_3g_loss.item())\n",
        "        losses_log[\"hairappearance_2g\"].append(hairappearance_2g_loss.item())\n",
        "        losses_log[\"hairappearance_3g\"].append(hairappearance_3g_loss.item())\n",
        "\n",
        "        n_loss = args.noise_regularize * noise_regularize(noises)\n",
        "        losses_log[\"noise\"].append(n_loss.item())\n",
        "\n",
        "        loss = (facerec_loss + n_loss)\n",
        "\n",
        "        if i < 1000:\n",
        "            loss += hairrec_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            loss += (hairstyle_3g_loss + hairappearance_3g_loss)\n",
        "\n",
        "            if args.use_GO == 0:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            else:\n",
        "                # Accumulate gradients from losses that do not participate in projection loss\n",
        "                # NOTE: Use clone to get .grad since it is referencing memory location\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                simple_L = latent_in.grad.clone()\n",
        "                simple_noises = [n.grad.clone() for n in noises]\n",
        "                \n",
        "                # Get gradient g_L\n",
        "                optimizer.zero_grad()\n",
        "                hairrec_loss.backward(retain_graph=True)\n",
        "                g_L = latent_in.grad.clone()\n",
        "                g_L_noises = [n.grad.clone() for n in noises]\n",
        "\n",
        "                # Get gradient g_S_2\n",
        "                optimizer.zero_grad()\n",
        "                (hairstyle_2g_loss + hairappearance_2g_loss).backward(retain_graph=True)\n",
        "                g_S2 = latent_in.grad.clone()\n",
        "                g_S2_noises = [n.grad.clone() for n in noises]\n",
        "\n",
        "                # Get gradient g_S_3\n",
        "                optimizer.zero_grad()\n",
        "                (hairstyle_3g_loss + hairappearance_3g_loss).backward(retain_graph=True)\n",
        "                g_S3 = latent_in.grad.clone()\n",
        "\n",
        "                # Compute gradient orthogonalization\n",
        "                # g_L, g_S_2  are [1, 18, 512] matrices\n",
        "                # <g_L, g_S_2 + > will do dot between <g_L[1, i, 512], g_S_2[1, i, 512]>\n",
        "                \n",
        "                # The following code does <g_L, g_S_2> / <g_S_2, g_S_2>\n",
        "                norm_vector = []\n",
        "                for w_pos in range(len(g_L[0])):\n",
        "                    g_S2_hat = F.normalize(g_S2[0, w_pos, :].unsqueeze(0), p=2)\n",
        "                    dot_L_S2 = torch.dot(g_L[0, w_pos, :], g_S2_hat.squeeze()) * g_S2_hat\n",
        "                    norm_vector.append(dot_L_S2)\n",
        "\n",
        "                norm_vector = torch.stack(norm_vector)  # [18 x 1 x 512]\n",
        "                norm_vector = torch.transpose(norm_vector, 0, 1)  # [1 x 18 x 512]\n",
        "\n",
        "                adjusted_g_L = g_L - norm_vector\n",
        "\n",
        "                # Record gradient norms\n",
        "                for idx in range(len(g_L[0])):\n",
        "                    g_S2_vector_norms[idx].append(torch.norm(g_S2[0, idx, :]).item())\n",
        "                    g_S3_vector_norms[idx].append(torch.norm(g_S3[0, idx, :]).item())\n",
        "                    g_L_vector_norms[idx].append(torch.norm(g_L[0, idx, :]).item())\n",
        "                    g_L_hat_vector_norms[idx].append(\n",
        "                        torch.norm(adjusted_g_L[0, idx, :]).item()\n",
        "                    )\n",
        "                    dot_gL_gS2[idx].append(\n",
        "                        torch.dot(g_L[0, idx, :], g_S2[0, idx, :]).item()\n",
        "                    )\n",
        "                    dot_gLhat_gS2[idx].append(\n",
        "                        torch.dot(adjusted_g_L[0, idx, :], g_S2[0, idx, :]).item()\n",
        "                    )\n",
        "\n",
        "                # Do update\n",
        "                optimizer.zero_grad()\n",
        "                loss = (\n",
        "                    facerec_loss\n",
        "                    + hairstyle_3g_loss\n",
        "                    + hairappearance_3g_loss\n",
        "                    + hairrec_loss\n",
        "                    + n_loss\n",
        "                )\n",
        "                loss.backward()\n",
        "\n",
        "                # assign precomputed statistics to gradient parameter\n",
        "                latent_in.grad = simple_L + adjusted_g_L\n",
        "\n",
        "                for idx in range(len(noises)):\n",
        "                    noises[idx].grad = noises[idx].grad - g_L_noises[idx]\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "        noise_normalize_(noises)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            latent_path.append(latent_in.detach().clone())\n",
        "\n",
        "        pbar.set_description(\n",
        "            (\n",
        "                f\"perc_face: {facerec_loss.item():.4f};\"\n",
        "                f\"perc_hair: {hairrec_loss.item():.4f};\"\n",
        "                f\"style_hair: {hairstyle_3g_loss.item():.4f};\"\n",
        "                f\"app_hair: {hairappearance_3g_loss.item():.4f};\"\n",
        "                f\"noise regularize: {n_loss.item():.4f};\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if (i + 1) % args.save_synth_every == 0:\n",
        "            image_utils.writeImageToDisk(\n",
        "                [I_G.clone()], [f'synth-{str(i)}.png'], dest\n",
        "            )\n",
        "            image_utils.writeMaskToDisk(\n",
        "                [HM_G, FM_G],\n",
        "                [f'synth_hair_mask-{str(i)}.png', f'synth_face_mask-{str(i)}.png'],\n",
        "                dest\n",
        "            )\n",
        "\n",
        "    img_gen, _ = g_ema([latent_path[-1]], input_is_latent=True, noise=noises)\n",
        "\n",
        "    noise_single = []\n",
        "    for noise in noises:\n",
        "        noise_single.append(noise[0:1].detach().cpu().numpy())\n",
        "\n",
        "    if args.save_pickle:\n",
        "        image_utils.writePickleToDisk(\n",
        "            [latent_in[0], noise_single, losses_log, g_S2_vector_norms,\n",
        "                g_S3_vector_norms, g_L_vector_norms, g_L_hat_vector_norms,\n",
        "                dot_gL_gS2, dot_gLhat_gS2],\n",
        "            [\"w_latent.pkl\", \"noises.pkl\", \"losses.pkl\", \"g_S2_vector_norms.pkl\",\n",
        "                \"g_S3_vector_norms.pkl\", \"g_L_vector_norms.pkl\", \"g_L_hat_vector_norms.pkl\",\n",
        "                \"dot_gL_gS2.pkl\", \"dot_gLhat_gS2.pkl\"],\n",
        "            dest\n",
        "        )\n",
        "\n",
        "    ########### INPAINT BACKGROUND\n",
        "\n",
        "    # Get softmask\n",
        "    with open(os.path.join(softmask, args.image1.split('.')[0] + '.pkl'), 'rb') as handle:\n",
        "        softmask = pickle.load(handle)\n",
        "\n",
        "    # Get inpainted background\n",
        "    background = cv2.imread(os.path.join(background, args.image1))\n",
        "    background = cv2.resize(background, (512, 512))\n",
        "\n",
        "    img_gen = image_utils.makeImage(img_gen)[0] # in RGB\n",
        "    img_gen = cv2.cvtColor(cv2.resize(img_gen, (512, 512)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    result = (softmask * img_gen) + (1 - softmask) * background\n",
        "    result = result.astype(np.uint8)\n",
        "    cv2.imwrite(os.path.join(dest, 'result.png'), result)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}